{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Model: CNN-ANN"
      ],
      "metadata": {
        "id": "Bct7XyxH_wLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Conv1D,\n",
        "    MaxPooling1D,\n",
        "    Flatten,\n",
        "    concatenate,\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Excel file\n",
        "nifty50_excel_file = 'combined_stock_data_single_sheet.xlsx'\n",
        "xls = pd.ExcelFile(nifty50_excel_file)\n",
        "\n",
        "# Define features (X)\n",
        "features = ['Open', 'High', 'Low', 'Adj Close', 'Volume',\n",
        "            'Previous_Close', '5SMA', '10SMA', '20SMA', '50SMA', '100SMA', '200SMA',\n",
        "            '5EMA', '10EMA', '20EMA', 'MACD', 'MACD_signal', 'RSI', 'PSAR',\n",
        "            'vortex_indicator', 'Upper_Band', 'Lower_Band', 'ATR5', 'ATR10',\n",
        "            'ATR20', 'ATR50', 'Stoch_Signal', 'Stoch', 'WR', 'TSI', 'ADX', 'VWAP',\n",
        "            'Daily_Return', 'Cumulative_Return', 'ROC5', 'ROC10', 'ROC20', 'ROC50',\n",
        "            'ROC100', 'ROC200', 'CMF', 'Daily_Log_Return']\n",
        "\n",
        "# Create an empty DataFrame to store results\n",
        "all_results = pd.DataFrame(columns=['Company', 'RMSE', 'MSE', 'MAE', 'R-squared', 'MAPE'])\n",
        "\n",
        "# Define the number of days to shift for future prediction (e.g., 10 days ahead)\n",
        "days_to_shift = 10\n",
        "\n",
        "# Loop through each sheet (company) in the Excel file\n",
        "for sheet_name in xls.sheet_names:\n",
        "    print(f\"Processing data for {sheet_name}...\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(nifty50_excel_file, sheet_name=sheet_name)\n",
        "\n",
        "        # Shift the 'Close' column to create the target variable for future prediction\n",
        "        df[f'Close_{days_to_shift}_Days_Ahead'] = df['Close'].shift(-days_to_shift)\n",
        "\n",
        "        # Drop rows with missing values (last rows where target is NaN)\n",
        "        df = df.dropna()\n",
        "\n",
        "        # Define the target variable (y) as 'Close_X_Days_Ahead'\n",
        "        target = f'Close_{days_to_shift}_Days_Ahead'\n",
        "\n",
        "        X = df[features]\n",
        "        y = df[target]\n",
        "\n",
        "        # Calculate the split point\n",
        "        split_point = int(len(X) * 0.8)  # 80% of the data for training, 20% for testing\n",
        "\n",
        "        X_train, X_test = X[:split_point], X[split_point:]\n",
        "        y_train, y_test = y[:split_point], y[split_point:]\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "\n",
        "        # Build the CNN part of the model for 1D data\n",
        "        cnn_input = Input(shape=(X_train.shape[1], 1))\n",
        "        cnn_layer1 = Conv1D(32, 3, activation='relu')(cnn_input)\n",
        "        cnn_layer1 = MaxPooling1D(2)(cnn_layer1)\n",
        "        cnn_layer2 = Conv1D(64, 3, activation='relu')(cnn_layer1)\n",
        "        cnn_layer2 = MaxPooling1D(2)(cnn_layer2)\n",
        "        cnn_flatten = Flatten()(cnn_layer2)\n",
        "\n",
        "        # Build the ANN part of the model\n",
        "        ann_input = Input(shape=(X_train.shape[1],))\n",
        "        ann_layer1 = Dense(128, activation='relu')(ann_input)\n",
        "        ann_layer1 = Dropout(0.2)(ann_layer1)\n",
        "        ann_layer2 = Dense(64, activation='relu')(ann_layer1)\n",
        "        ann_layer2 = Dropout(0.2)(ann_layer2)\n",
        "\n",
        "        # Concatenate the CNN and ANN parts\n",
        "        combined = concatenate([cnn_flatten, ann_layer2])\n",
        "\n",
        "        # Create the final model for regression\n",
        "        output = Dense(1, activation='linear')(combined)\n",
        "        model = Model(inputs=[cnn_input, ann_input], outputs=output)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "        # Train the model\n",
        "        model.fit([X_train, X_train], y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred = model.predict([X_test, X_test])\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r_squared = r2_score(y_test, y_pred)\n",
        "        mape = np.mean(np.abs((y_test.values - y_pred.flatten()) / y_test.values)) * 100\n",
        "\n",
        "        # Store results in the DataFrame\n",
        "        result = pd.DataFrame({\n",
        "            'Company': [sheet_name],\n",
        "            'RMSE': [rmse],\n",
        "            'MSE': [mse],\n",
        "            'MAE': [mae],\n",
        "            'R-squared': [r_squared],\n",
        "            'MAPE': [mape]\n",
        "        })\n",
        "\n",
        "        all_results = all_results.append(result, ignore_index=True)\n",
        "\n",
        "        print(f\"Data for {sheet_name} processed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data for {sheet_name}: {str(e)}\")\n",
        "\n",
        "# Save the results to a CSV file\n",
        "all_results.to_csv('ann_cnn_results.csv', index=False)\n",
        "\n",
        "print('Results saved to ann_cnn_results.csv')\n"
      ],
      "metadata": {
        "id": "5CqDXSGCEToz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}